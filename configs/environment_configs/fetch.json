{
    "env_kwargs": {},

    "wrappers": ["env_utils.wrappers.CustomResetFnWrapper", "env_utils.wrappers.SparseRewardWrapper"],
    "wrapper_kwargs": [
        {"env_initialization_fn": "env_utils.helpers.default.initialization_fn"}, 
        {"reward_threshold": -0.05}
    ],

    "eval_wrappers": ["env_utils.wrappers.CustomResetFnWrapper", "env_utils.wrappers.SparseRewardWrapper"],
    "eval_wrapper_kwargs": [
        {"env_initialization_fn": "env_utils.helpers.default.initialization_fn"}, 
        {"reward_threshold": -0.05}
    ],

    "shaper_kwargs": {
        "shaping_preprocess_obs_fn": "env_utils.helpers.fetch.preprocess_obs_fn",
        "inference_preprocess_obs_fn": "env_utils.helpers.fetch.preprocess_obs_fn"
    },

    "learner_kwargs": {
        "policy": "MultiInputPolicy",
        "replay_buffer_class": "utils.replay_buffer.ChangingRewardHerReplayBuffer",
        "learning_rate": 0.001,
        "batch_size": 1024,
        "policy_kwargs": {
            "net_arch": [512, 512, 512]
        },
        "gamma": 0.9,
        "tau": 0.05
    },

    "train_kwargs": {
        "total_timesteps": 500000
    },

    "eval_kwargs": {
        "eval_freq": 5000
    }
}